==== PROMPT ====

           **EXAMPLE RESUME**:
    """
        ==== Page 1 ====
              GABRIEL BAKER
              Data Architect | Azure Expert
               1  234  555 1234
              gabriel@gmail.com
              linkedin.com
              Jacksonville, Florida
              Extra Field
              Summary
              With over 8 years of experience as a Data Architect, I specialize in Azure Data Platform and data modeling, achieving a 
              30% increase in data efficiency at a top firm. Committed to pioneering state-of-the-art solutions and enhancing data 
              strategies.
              Skills
              SQL, Azure Databricks, Python, Data Modeling, Azure Data Platform, ETL Pipelines
              Experience
              Oracle Corporation
              Orlando, FL
              Senior Data Architect
              06/2019   08/2023
              •
              Led a team of 6 to design and implement a scalable data architecture, improving processing speed by 40% and 
              reducing delay in reporting.
              Collaborated with cross-departmental teams to integrate new data sources, resulting in a 30% data comprehensiveness 
              improvement.
              Developed new data modeling techniques that decreased data retrieval time by 15%, enhancing overall team efficiency.
              Effectively managed Azure Databricks-based data environments, ensuring optimal performance and a 99.9% uptime 
              over 18 months.
              Played a pivotal role in strategic data projects that resulted in a 25% boost in user satisfaction scores due to improved 
              data handling.
              Designed a compliance tracking system using Python and SQL, ensuring all data processes adhered to regulatory 
              standards.
              •
              •
              •
              •
              •
              IBM
              Raleigh, NC
              Data Solutions Architect
              03/2015   05/2019
              ••••
              •
              Design and executed a data migration strategy, saving in storage costs and reducing operational time by 20%.
              Partnered with data analysts to automate 50% of reports, improving accuracy and saving 10 hours of labor weekly.
              Enhanced data security measures across cloud platforms, minimizing breach risks and ensuring data integrity.
              Implemented machine learning algorithms for predictive analytics, increasing forecasting accuracy by 18%.
              Consulted with senior management to draft and implement data strategies for business growth, resulting in a 15% 
              revenue increase in key areas.
              Accenture
              Tampa, FL
              Data Engineer
              01/2012   02/2015
              ••••
              Engineered ETL pipelines, enhancing data flow efficiency and reducing processing delays by 25%.
              Optimized SQL databases, contributing to a 30% improvement in query performance and resource utilization.
              Contributed to a cross-functional team to develop a new data analytics platform with a 98% uptime.
              Identified and resolved data discrepancies, maintaining data accuracy and earning a 95% quality assurance rating.
              Education
              University of Florida
              Gainesville, FL
              Master of Science in Data Science
              01/2010   01/2012
              University of Central Florida
              Orlando, FL
              Bachelor of Science in Computer Science
              01/2006   01/2010
              www.enhancv.com
              
              Powered by
    
              ==== Page 2 ====
              Projects
              Open Source Data Connector
              Developed a Python-based data connector for Azure services to enhance integration capabilities. 
              github.com/gabrielbakerdata/dataconnector
              Real-time Analytics Dashboard
              Built a dashboard using PowerBI and Azure to provide real-time operational analytics. 
              github.com/gabrielbakerdata/analyticsdashboard
              www.enhancv.com
              
              Powered by
        """
    
        **EXAMPLE OUTPUT**:
        {
    
        "Name": "GABRIEL BAKER",
        "Email": "gabriel@gmail.com",
        "Phone": " 1  234  555 1234",
        "Location": "Jacksonville, Florida",
        "LinkedIn": "linkedin.com",
        "Education": [
          {
            "Degree": "Master of Science in Data Science",
            "University": "University of Florida",
            "City/Location": "Gainesville, FL",
            "Duration": "01/2010   01/2012"
          },
          {
            "Degree": "Bachelor of Science in Computer Science",
            "University": "University of Central Florida",
            "City/Location": "Orlando, FL",
            "Duration": "01/2006   01/2010"
          }
        ],
        "Experience": [
          {
            "Title": "Senior Data Architect",
            "Company": "Oracle Corporation",
            "Duration": "06/2019   08/2023",
            "Location": "Orlando, FL",
            "Description": [
              "Led a team of 6 to design and implement a scalable data architecture, improving processing speed by 40% and reducing delay in reporting.",
              "Collaborated with cross-departmental teams to integrate new data sources, resulting in a 30% data comprehensiveness improvement.",
              "Developed new data modeling techniques that decreased data retrieval time by 15%, enhancing overall team efficiency.",
              "Effectively managed Azure Databricks-based data environments, ensuring optimal performance and a 99.9% uptime over 18 months.",
              "Played a pivotal role in strategic data projects that resulted in a 25% boost in user satisfaction scores due to improved data handling.",
              "Designed a compliance tracking system using Python and SQL, ensuring all data processes adhered to regulatory standards."
            ]
          },
          {
            "Title": "Data Solutions Architect",
            "Company": "IBM",
            "Duration": "03/2015   05/2019",
            "Location": "Raleigh, NC",
            "Description": [
              "Design and executed a data migration strategy, saving in storage costs and reducing operational time by 20%.",
              "Partnered with data analysts to automate 50% of reports, improving accuracy and saving 10 hours of labor weekly.",
              "Enhanced data security measures across cloud platforms, minimizing breach risks and ensuring data integrity.",
              "Implemented machine learning algorithms for predictive analytics, increasing forecasting accuracy by 18%.",
              "Consulted with senior management to draft and implement data strategies for business growth, resulting in a 15% revenue increase in key areas."
            ]
          },
          {
            "Title": "Data Engineer",
            "Company": "Accenture",
            "Duration": "01/2012   02/2015",
            "Location": "Tampa, FL",
            "Description": [
              "Engineered ETL pipelines, enhancing data flow efficiency and reducing processing delays by 25%.",
              "Optimized SQL databases, contributing to a 30% improvement in query performance and resource utilization.",
              "Contributed to a cross-functional team to develop a new data analytics platform with a 98% uptime.",
              "Identified and resolved data discrepancies, maintaining data accuracy and earning a 95% quality assurance rating."
            ]
          }
        ],
        "Skills": [
          "SQL",
          "Azure Databricks",
          "Python",
          "Data Modeling",
          "Azure Data Platform",
          "ETL Pipelines"
        ],
        }
    
        Your Task is to Extract structred data from the 8 fields mentioned bellow follow from resumes. Comply with every rule—no assumptions, no inference, no paraphrasing, no missing fields.
Your response is a valid Json output.
Return a flat JSON object with exactly these 8 top-level fields:
    1. Name  
    2. Phone  
    3. Email (optional — return [] if missing)  
    4. Location  
    5. LinkedIn (optional — return [] if missing)  
    6. Education (list of):
        - Degree  
        - University  
        - City/Location  
        - Duration (Start/End or range)
    7. Experience (list for each job):
        - Title  
        - Company  
        - Duration  
        - Location  
        - Full Description (verbatim from resume — preserve bullet points and formatting; do not paraphrase)
    8. Skills (list of individual strings; include all technical terms, even short ones like "SQL", "R", or "C++")

CRITICAL RULES:

1. **STRICT FIELD LIMITATION**
   - Do NOT extract or return any other fields.
   - Ignore: Summary, Certifications, Awards, Projects, Interests, Hobbies, GPA, Classes, Honors, etc.

2. **ALIAS MAPPING**
   Accept alternate section headings:
   - "Technical Skills", "Skills Summary", "Tools" → "skills"
   - "Professional Experience", "Work History" → "experience"
   - "Academic Background", "Education History" → "education"

3. **UNREADABLE CHARACTER RULE**
   - Replace every occurrence of � with “[UNK]”
   - Do NOT guess what the character was

   Example:
   Input: �1��234��555�
   Output: [UNK]1[UNK]234[UNK]555[UNK]

4. **MISSING FIELD HANDLING**
   - If any field is missing, output:
     - `""` for strings
     - `[]` for lists

5. **SOURCE STRUCTURE**
   - Do not rearrange content
   - Do not infer structure or context
   - Never interpret a line unless it exactly matches the schema

6. **JSON OUTPUT FORMAT**
   - Output a single valid JSON object ONLY
   - No markdown, no explanations, no field names outside the schema
   - Must be 100% JSON-parseable
   - Top-level keys must match schema exactly: name, location, linkedin, phone, mail, experience, education, skills
   - Do not add any other fields under any circumstance. Violating this will be considered an invalid output.
 **Resume**:
            """
            ==== Page 1 ====
ZOE THOMPSON
Data Warehouse Engineer | Data Analytics | Cloud Solutions
�1��234��555�1234
zoe@gmail.com
linkedin.com
Los Angeles, California
E

q

SUMMARY
With 10� years of experience as a data warehouse engineer, I bring extensive expertise in data modeling, cloud data solutions, 
and analytics tools such as SQL, Snowflake, and PowerBI. My contributions have substantively improved operational efficiencies 
and informed strategic business decisions, evidenced by a marked increase in productivity and profitability.
SKILLS
Data Warehousing, Data Modeling, ERWin, SQL, Snowflake, Redshift
EXPERIENCE
Lead Data Warehouse Architect
TechSolutions Inc
06/2019 � Present 
Los Angeles, CA
•
Led the migration of on-premises legacy data systems to cloud-based warehouses, achieving a 40% reduction in operational 
costs while improving data retrieval times by 70%.
Spearheaded the integration of Snowflake cloud data warehouse which enhanced data analytics and reporting capabilities for 
cross-functional teams, supporting an increase in data-driven decision-making.
Implemented a new E�R modeling system for efficient database structuring that streamlined data flow and reduced 
redundancy, resulting in a 25% increase in productivity for the data analytics team.
Designed and maintained over 20 data marts, enabling tailored data views which facilitated user-specific analysis, greatly 
enhancing the usability of data resources organization-wide.
Automated the extraction, transformation, and load processes �ETL� using Python and SQL, leading to a 50% reduction in the 
time required to prepare data for analysis.
Developed a comprehensive reporting system using PowerBI and Tableau that provided actionable insights into customer 
behavior, driving a 15% increase in targeted marketing campaign success.
•
•
•
•
•
Senior Data Engineer
Innovatech Data Solutions
03/2016 � 05/2019 
Los Angeles, CA
•
Optimized the performance of data warehouses by implementing indexing strategies and query optimizations that resulted in a 
30% improvement in data processing speeds.
Collaborated with cross-functional teams to translate business requirements into technical data solutions, aiding in the launch 
of three major product lines.
Managed the lifecycle of big data solutions, including building and maintaining NoSQL databases such as Cosmos and Bigtable, 
which handled over 40 TB of data.
Devised data validation protocols and error identification mechanisms to ensure a 99.9% accuracy rate in data reporting.
Initiated and led the deployment of an automated data quality system that reduced manual error correction by 75%, 
significantly increasing data reliability for end-users.
•
•
•
•
Data Warehouse Specialist
QuantTech Analytics
08/2012 � 02/2016 
San Francisco, CA
•
Contributed to the design and development of a multi-terabyte data warehouse that consolidated data across different 
departments, facilitating a holistic view of the business.
Enhanced ETL workflows which reduced the data processing window by 4 hours, enabling more timely data availability for 
analysis.
Trained 20� staff members on best practices for data manipulation, query optimization, and report generation, improving 
overall team efficiency.
Conducted in-depth analyses of market trends and customer data that contributed to a strategic shift in sales focus, increasing 
revenue by 10%.
•
•
•
EDUCATION
Master of Science in Data Science
University of Southern California
01/2010 � 01/2012 
Los Angeles, CA
www.enhancv.com

Powered by

==== Page 2 ====
EDUCATION
Bachelor of Science in Computer Science
University of California, Los Angeles
01/2006 � 01/2010 
Los Angeles, CA
PROJECTS
Real-time Data Parsing Engine
Developed a Python-based engine for processing streaming data into actionable insights, while ensuring minimal latency. GitHub 
link: github.com/ZoeThompson/RealTimeDataParser
Custom ETL Framework
Created an extensible ETL framework to standardize data transformation tasks across various projects, improving code 
reusability. GitHub link: github.com/ZoeThompson/CustomETLFramework
www.enhancv.com

Powered by
            """    
        **Resume**:
        """
        ==== Page 1 ====
ZOE THOMPSON
Data Warehouse Engineer | Data Analytics | Cloud Solutions
�1��234��555�1234
zoe@gmail.com
linkedin.com
Los Angeles, California
E

q

SUMMARY
With 10� years of experience as a data warehouse engineer, I bring extensive expertise in data modeling, cloud data solutions, 
and analytics tools such as SQL, Snowflake, and PowerBI. My contributions have substantively improved operational efficiencies 
and informed strategic business decisions, evidenced by a marked increase in productivity and profitability.
SKILLS
Data Warehousing, Data Modeling, ERWin, SQL, Snowflake, Redshift
EXPERIENCE
Lead Data Warehouse Architect
TechSolutions Inc
06/2019 � Present 
Los Angeles, CA
•
Led the migration of on-premises legacy data systems to cloud-based warehouses, achieving a 40% reduction in operational 
costs while improving data retrieval times by 70%.
Spearheaded the integration of Snowflake cloud data warehouse which enhanced data analytics and reporting capabilities for 
cross-functional teams, supporting an increase in data-driven decision-making.
Implemented a new E�R modeling system for efficient database structuring that streamlined data flow and reduced 
redundancy, resulting in a 25% increase in productivity for the data analytics team.
Designed and maintained over 20 data marts, enabling tailored data views which facilitated user-specific analysis, greatly 
enhancing the usability of data resources organization-wide.
Automated the extraction, transformation, and load processes �ETL� using Python and SQL, leading to a 50% reduction in the 
time required to prepare data for analysis.
Developed a comprehensive reporting system using PowerBI and Tableau that provided actionable insights into customer 
behavior, driving a 15% increase in targeted marketing campaign success.
•
•
•
•
•
Senior Data Engineer
Innovatech Data Solutions
03/2016 � 05/2019 
Los Angeles, CA
•
Optimized the performance of data warehouses by implementing indexing strategies and query optimizations that resulted in a 
30% improvement in data processing speeds.
Collaborated with cross-functional teams to translate business requirements into technical data solutions, aiding in the launch 
of three major product lines.
Managed the lifecycle of big data solutions, including building and maintaining NoSQL databases such as Cosmos and Bigtable, 
which handled over 40 TB of data.
Devised data validation protocols and error identification mechanisms to ensure a 99.9% accuracy rate in data reporting.
Initiated and led the deployment of an automated data quality system that reduced manual error correction by 75%, 
significantly increasing data reliability for end-users.
•
•
•
•
Data Warehouse Specialist
QuantTech Analytics
08/2012 � 02/2016 
San Francisco, CA
•
Contributed to the design and development of a multi-terabyte data warehouse that consolidated data across different 
departments, facilitating a holistic view of the business.
Enhanced ETL workflows which reduced the data processing window by 4 hours, enabling more timely data availability for 
analysis.
Trained 20� staff members on best practices for data manipulation, query optimization, and report generation, improving 
overall team efficiency.
Conducted in-depth analyses of market trends and customer data that contributed to a strategic shift in sales focus, increasing 
revenue by 10%.
•
•
•
EDUCATION
Master of Science in Data Science
University of Southern California
01/2010 � 01/2012 
Los Angeles, CA
www.enhancv.com

Powered by

==== Page 2 ====
EDUCATION
Bachelor of Science in Computer Science
University of California, Los Angeles
01/2006 � 01/2010 
Los Angeles, CA
PROJECTS
Real-time Data Parsing Engine
Developed a Python-based engine for processing streaming data into actionable insights, while ensuring minimal latency. GitHub 
link: github.com/ZoeThompson/RealTimeDataParser
Custom ETL Framework
Created an extensible ETL framework to standardize data transformation tasks across various projects, improving code 
reusability. GitHub link: github.com/ZoeThompson/CustomETLFramework
www.enhancv.com

Powered by
        """
        

==== MODEL OUTPUT ====
[No output received]